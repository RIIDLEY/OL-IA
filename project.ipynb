{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e7aaaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Étape 1. Importez les bibliothèques nécessaires, nous allons utiliser plus loin\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72b2f956",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                Annas Dream Full Quilt with 2 Shams   \n",
       "1  Stop Pacifier Sucking without tears with Thumb...   \n",
       "2  Stop Pacifier Sucking without tears with Thumb...   \n",
       "\n",
       "                                              review sentiment  \n",
       "0  Very soft and comfortable and warmer than it l...  positive  \n",
       "1  This is a product well worth the purchase.  I ...  positive  \n",
       "2  All of my kids have cried non-stop when I trie...  positive  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Étape 2. Lecture du fichier .txt de Pandas et afficher les 3 premières lignes\n",
    "df = pd.read_table('dataset.txt', sep=';')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "279b0780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name          3\n",
       "review       23\n",
       "sentiment     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Étape 3. Nettoyer les données textuelles\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c692abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name         0\n",
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#supprimer les valeurs null\n",
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de781fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#installer nltk \n",
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3b131d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tester nltk \n",
    "#from nltk.corpus import brown\n",
    "#brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "681238c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init Objects\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stopwords=set(stopwords.words('english'))\n",
    "ps=PorterStemmer()\n",
    "def getStemmedReview(review):\n",
    "    review=review.lower()\n",
    "    review=review.replace(\"<br /><br />\",\" \")\n",
    "    #Tokenize\n",
    "    tokens=tokenizer.tokenize(review)\n",
    "    new_tokens=[token for token in tokens if token not in  en_stopwords]\n",
    "    stemmed_tokens=[ps.stem(token) for token in new_tokens]\n",
    "    clean_review=' '.join(stemmed_tokens)\n",
    "    return clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b39f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Étape 4. Nettoyer toutes les revues et diviser nos données pour la formation et les tests.\n",
    "df['review'].apply(getStemmedReview)\n",
    "X_train = df.loc[:8000, 'review'].values\n",
    "y_train = df.loc[:8000, 'sentiment'].values\n",
    "X_test = df.loc[2000:, 'review'].values\n",
    "y_test = df.loc[2000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installer : pip3 install -U scikit-learn scipy matplotlib ou pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "192400cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Étape 5. Transformer des mots en vecteurs de caractéristiques\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, encoding='utf-8',\n",
    " decode_error='ignore')\n",
    "vectorizer.fit(X_train)\n",
    "X_train=vectorizer.transform(X_train)\n",
    "X_test=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92f7c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data is: 0.9149442565451584\n",
      "Score on testing data is: 0.9067785991730359\n"
     ]
    }
   ],
   "source": [
    "#Étape 6. Création du modèle et vérification du score sur les données d'entraînement et de test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model=LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "print(\"Score on training data is: \"+str(model.score(X_train,y_train)))\n",
    "print(\"Score on testing data is: \"+str(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "393b62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester sur une nouvelle valeur \n",
    "newval = [\"While the diaper pins are attractive, the metal in the pins I received are flimsy and did not hold up to the thick fabric I used them on. Fortunately there was no baby involved\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4bf8beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "newval=vectorizer.transform(newval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a5ff4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(newval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "64e01cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6765112, 0.3234888]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(newval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce04925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
