{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7aaaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Étape 1. Importez les bibliothèques nécessaires, nous allons utiliser plus loin\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Étape 2. Lecture du fichier .txt de Pandas et afficher les 3 premières lignes\n",
    "df = pd.read_table('dataset.txt', sep=';')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b0780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Étape 3. Nettoyer les données textuelles\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c692abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#supprimer les valeurs null\n",
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de781fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installer nltk \n",
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b131d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tester nltk \n",
    "#from nltk.corpus import brown\n",
    "#brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681238c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init Objects\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stopwords=set(stopwords.words('english'))\n",
    "ps=PorterStemmer()\n",
    "def getStemmedReview(review):\n",
    "    review=review.lower()\n",
    "    review=review.replace(\"<br /><br />\",\" \")\n",
    "    #Tokenize\n",
    "    tokens=tokenizer.tokenize(review)\n",
    "    new_tokens=[token for token in tokens if token not in  en_stopwords]\n",
    "    stemmed_tokens=[ps.stem(token) for token in new_tokens]\n",
    "    clean_review=' '.join(stemmed_tokens)\n",
    "    return clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Étape 4. Nettoyer toutes les revues et diviser nos données pour la formation et les tests.\n",
    "df['review'].apply(getStemmedReview)\n",
    "X_train = df.loc[:8000, 'review'].values\n",
    "y_train = df.loc[:8000, 'sentiment'].values\n",
    "X_test = df.loc[2000:, 'review'].values\n",
    "y_test = df.loc[2000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installer : pip3 install -U scikit-learn scipy matplotlib ou pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192400cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Étape 5. Transformer des mots en vecteurs de caractéristiques\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, encoding='utf-8',\n",
    " decode_error='ignore')\n",
    "vectorizer.fit(X_train)\n",
    "X_train=vectorizer.transform(X_train)\n",
    "X_test=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Étape 6. Création du modèle et vérification du score sur les données d'entraînement et de test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model=LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "print(\"Score on training data is: \"+str(model.score(X_train,y_train)))\n",
    "print(\"Score on testing data is: \"+str(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "393b62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester sur une nouvelle valeur \n",
    "newval = [\"While the diaper pins are attractive, the metal in the pins I received are flimsy and did not hold up to the thick fabric I used them on. Fortunately there was no baby involved\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4bf8beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "newval=vectorizer.transform(newval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a5ff4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(newval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "64e01cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6765112, 0.3234888]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(newval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce04925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
